{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79fb14d6-4f68-4e19-9512-1ff2a6f9cd1f",
   "metadata": {},
   "source": [
    " # üìå PipeLine para descarga y atomizaci√≥n de Datos\n",
    "\n",
    "üîπEste documento describe la arquitectura utilizada para descargar datos crudos desde KoboToolbox, almacenarlos en Google Cloud Storage, y posteriormente atomizarlos y cargarlos en BigQuery mediante Cloud Functions.\n",
    "\n",
    "\n",
    "-------------------\n",
    "### üìÅ Perfiladores de Corriente \n",
    "- La estructura de Cloud Functions requiere de dos archivos: **main.py** y **requirement.txt**\n",
    "- *main.py* - C√≥digo con la funci√≥n para acceder a los datos de la aplicaci√≥n KoboToolbox y guardar los registros (datos crudos) en un Bucket de *Cloud Storage*\n",
    "- *requirement.txt* - Debe de contener:\n",
    "\n",
    " \n",
    "\n",
    "1) requests ‚Üí Para hacer solicitudes HTTP a la API de KoBoToolbox.\n",
    "\n",
    "2) google.cloud.storage ‚Üí Para conectarse al Bucket de Cloud Storage.\n",
    "\n",
    "3) functions_framework ‚Üí Permite ejecutar la funci√≥n como Cloud Function.\n",
    "\n",
    "Para configurar la funci√≥n en Google Cloud Functions (GCF), es necesario establecer las variables de entorno requeridas por el c√≥digo principal, lo que simplifica su despliegue y mantenimiento.\n",
    "\n",
    "- KOBO_TOKEN= \"d0e1084994983390bd6f50e3ee61e9c522aee152\"\n",
    "- BUCKET_NAME= Perfiladores_de_Corriente\n",
    "\n",
    "#### üöÄ C√≥digo *main.py*\n",
    "```` python\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "import functions_framework\n",
    "\n",
    "@functions_framework.http # Permite que la funci√≥n se ejecute cuando recibe una petici√≥n web y convierte la funci√≥n en una Cloud Function HTTP\n",
    "def main(request):\n",
    "    \n",
    "    # --- Configuraci√≥n ---\n",
    "    KOBO_URL = \"https://kf.kobotoolbox.org/api/v2/assets/akth74ZsstPt82WXhLUKmN/data.json\" #Endpoint espec√≠fico de la API de KoboToolbox para un asset/proyecto\n",
    "    KOBO_TOKEN = os.getenv(\"KOBO_TOKEN\")\n",
    "    BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "\n",
    "    # --- Descargar datos desde KoboToolbox ---\n",
    "    headers = {\"Authorization\": f\"Token {KOBO_TOKEN}\"} # Configura autenticaci√≥n Bearer Token para la API\n",
    "    response = requests.get(KOBO_URL, headers=headers) # Ejecuta la petici√≥n GET y almacena la respuesta\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return f\"Error {response.status_code}: {response.text}\", response.status_code\n",
    "\n",
    "    data = response.json() # Convertir respuesta a JSON\n",
    "\n",
    "    # --- Nombrar archivo por fecha ---\n",
    "    fecha = datetime.utcnow().strftime(\"%Y-%m-%dT%H%M%SZ\") # Se obtiene la fecha y se le da formato\n",
    "    filename = f\"kobo_raw_{fecha}.json\" # Se nombra el archivo\n",
    "\n",
    "    # --- Guardar archivo en Cloud Storage ---\n",
    "    \n",
    "    storage_client = storage.Client() # Crea una instancia del cliente oficial de Python para interactuar con Google Cloud Storage \n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob(filename) # Representa el archivo que se crear√° en el bucket\n",
    "    blob.upload_from_string(json.dumps(data), content_type=\"application/json\")\n",
    "\n",
    "    print(f\"Archivo subido: gs://{BUCKET_NAME}/{filename}\")\n",
    "    return f\"Datos guardados en {filename}\", 200\n",
    "\n",
    "````\n",
    "\n",
    "üîπPara realizar la automatizaci√≥n de la descarga de datos se tiene que declarar un mensaje de activaci√≥n, el disparador (trigger) y por ultimo se declaran las condiciones para que se ejecute en automatico. \n",
    "\n",
    "1. Mensaje de activaci√≥n funciona como intermediario entre Scheduler y Cloud Function:\\\n",
    "   gcloud pubsub topics create kobo-extractor-trigger\n",
    "   \n",
    "2. Trrigger despliega una funci√≥n que se ejecuta autom√°ticamente cuando llega un mensaje al topic: \\\n",
    "   gcloud functions deploy extraer_datos_kobo \\\n",
    "  --runtime python311 \\\n",
    "  --trigger-topic kobo-extractor-trigger \\\n",
    "  --region=us-central1\n",
    "\n",
    "   \n",
    "4. Programar la tarea: \\\n",
    "   gcloud scheduler jobs create pubsub ejecutar-extraccion-kobo \\\n",
    "  --schedule=\"0 8 1 * *\" \\\n",
    "  --topic=kobo-extractor-trigger \\\n",
    "  --message-body=\"Iniciar extracci√≥n de KoboToolbox\" \\\n",
    "  --time-zone=\"America/Mexico_City\"\n",
    "\n",
    "\n",
    "----------------------\n",
    "Una vez generada la dase de datos cruda en Cloud Storage se despliega otra funci√≥n donde a partir de los datos crudos se haga la atomizaci√≥n de los registros y se genere el Query. Su respectivo requirements.txt debe de contener: \n",
    "1) google-cloud-storage\n",
    "2) google-cloud-bigquery\n",
    "\n",
    "\n",
    "#### üöÄ C√≥digo *main.py*\n",
    "```` python\n",
    "    import json\n",
    "    from google.cloud import storage, bigquery\n",
    "\n",
    "    def atomizar_y_cargar(event, context):\n",
    "        \"\"\"Se ejecuta autom√°ticamente cuando se sube un nuevo archivo al bucket.\"\"\"\n",
    "        bucket_name = event['bucket']\n",
    "        file_name = event['name']\n",
    "        print(f\"Nuevo archivo detectado: gs://{bucket_name}/{file_name}\")\n",
    "\n",
    "        # --- Leer el archivo desde el bucket ---\n",
    "        \n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name) \n",
    "        blob = bucket.blob(file_name) # Referencia al archivo espec√≠fico\n",
    "        raw_data = blob.download_as_text()\n",
    "        json_data = json.loads(raw_data) #Convierte el texto JSON a objeto Python\n",
    "\n",
    "        # --- Extraer los registros del JSON ---\n",
    "        registros = json_data.get(\"results\", [])\n",
    "\n",
    "        # --- Limpiar y estructurar los datos ---\n",
    "        data_limpia = []\n",
    "        \n",
    "        for item in registros:\n",
    "            # Extraer datos del grupo \"group_dr3zv09\" (Informaci√≥n general)\n",
    "            grupo_principal = item.get(\"group_dr3zv09\", {})\n",
    "        \n",
    "            # Extraer datos del grupo \"group_jf5li71\" (Informaci√≥n del sensor)\n",
    "            grupo_sensor = item.get(\"group_jf5li71\", {})\n",
    "            data_limpia.append({\n",
    "                \n",
    "                # --- Informaci√≥n general (grupo_dr3zv09) ---\n",
    "                \"nombre_completo\": grupo_principal.get(\"Nombre_Completo\"),\n",
    "                \"fecha_recoleccion\": grupo_principal.get(\"Fecha_y_hora_de_recolecci_n\"),\n",
    "                \"reserva\": grupo_principal.get(\"reserva\"),\n",
    "                \"sitio\": grupo_principal.get(\"sitio\"),\n",
    "                \"codigo_sitio\": grupo_principal.get(\"codigo\"),\n",
    "                \n",
    "                # --- Informaci√≥n del sensor (grupo_jf5li71) ---\n",
    "                \"coordenadas_sensor\": coordenadas,\n",
    "                \"frecuencia_operacion\": grupo_sensor.get(\"Frecuencia_de_operaci_n\"),\n",
    "\n",
    "        })\n",
    "\n",
    "\n",
    "        # --- Subir los datos a BigQuery ---\n",
    "        bq_client = bigquery.Client()\n",
    "        table_id = \"Perfiladores_atomizado\"\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_APPEND\",  # agrega datos nuevos sin borrar los anteriores\n",
    "        autodetect=True # BigQuery infiere autom√°ticamente el esque de datos\n",
    "        )\n",
    "\n",
    "        job = bq_client.load_table_from_json(data_limpia, table_id, job_config=job_config)\n",
    "        job.result()  # Esperar a que finalice la carga, dtiene la ejecuci√≥n en CloudFunctions\n",
    "\n",
    "        print(f\"{len(data_limpia)} registros cargados a {table_id}\")\n",
    "\n",
    "````\n",
    "\n",
    "Desde Terminal se gener√° el ***Disparador*** de la funci√≥n para que se relice de forma automatica cada vez que se detecten nuevos datos en el bucket\n",
    "\n",
    "````pyhton \n",
    "gcloud functions deploy atomizar_y_cargar \\\n",
    "  --runtime python311 \n",
    "  --trigger-resource Perfiladores_de_Corriente \\\n",
    "  --trigger-event google.storage.object.finalize \\\n",
    "  --region=us-central1\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934dc5b6-ef8a-47dd-99ae-3f77e60581dd",
   "metadata": {},
   "source": [
    "## üìÅ Par√°metros de Agua\n",
    "--------\n",
    "\n",
    "### ‚ö° Descarga de datos crudos\n",
    "- *requirement.txt* \n",
    "\n",
    " \n",
    "\n",
    "1) requests ‚Üí Para hacer solicitudes HTTP a la API de KoBoToolbox.\n",
    "\n",
    "2) google.cloud.storage ‚Üí Para conectarse al Bucket de Cloud Storage.\n",
    "\n",
    "3) functions_framework ‚Üí Permite ejecutar la funci√≥n como Cloud Function.\n",
    "\n",
    "Para configurar la funci√≥n en Google Cloud Functions (GCF), es necesario establecer las variables de entorno requeridas por el c√≥digo principal, lo que simplifica su despliegue y mantenimiento.\n",
    "\n",
    "- KOBO_TOKEN= \"d0e1084994983390bd6f50e3ee61e9c522aee152\"\n",
    "- BUCKET_NAME= Par√°metros_de_Agua\n",
    "#### üöÄ C√≥digo *main.py*\n",
    "```` python\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "import functions_framework\n",
    "\n",
    "@functions_framework.http # Permite que la funci√≥n se ejecute cuando recibe una petici√≥n web y convierte la funci√≥n en una Cloud Function HTTP\n",
    "def main(request):\n",
    "    \n",
    "    # --- Configuraci√≥n ---\n",
    "    KOBO_URL = \"https://kf.kobotoolbox.org/api/v2/assets/aZcdmftcBtnpfW8HvV78as/data.json\" #Endpoint espec√≠fico de la API de KoboToolbox para un asset/proyecto\n",
    "    KOBO_TOKEN = os.getenv(\"KOBO_TOKEN\")\n",
    "    BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "\n",
    "    # --- Descargar datos desde KoboToolbox ---\n",
    "    headers = {\"Authorization\": f\"Token {KOBO_TOKEN}\"} # Configura autenticaci√≥n Bearer Token para la API\n",
    "    response = requests.get(KOBO_URL, headers=headers) # Ejecuta la petici√≥n GET y almacena la respuesta\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return f\"Error {response.status_code}: {response.text}\", response.status_code\n",
    "\n",
    "    data = response.json() # Convertir respuesta a JSON\n",
    "\n",
    "    # --- Nombrar archivo por fecha ---\n",
    "    fecha = datetime.utcnow().strftime(\"%Y-%m-%dT%H%M%SZ\") # Se obtiene la fecha y se le da formato\n",
    "    filename = f\"kobo_raw_{fecha}.json\" # Se nombra el archivo\n",
    "\n",
    "    # --- Guardar archivo en Cloud Storage ---\n",
    "    \n",
    "    storage_client = storage.Client() # Crea una instancia del cliente oficial de Python para interactuar con Google Cloud Storage \n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob(filename) # Representa el archivo que se crear√° en el bucket\n",
    "    blob.upload_from_string(json.dumps(data), content_type=\"application/json\")\n",
    "\n",
    "    print(f\"Archivo subido: gs://{BUCKET_NAME}/{filename}\")\n",
    "    return f\"Datos guardados en {filename}\", 200\n",
    "\n",
    "````\n",
    "\n",
    "üîπPara realizar la automatizaci√≥n de la descarga de datos se tiene que declarar un mensaje de activaci√≥n, el disparador (trigger) y por ultimo se declaran las condiciones para que se ejecute en automatico. \n",
    "\n",
    "1. Mensaje de activaci√≥n funciona como intermediario entre Scheduler y Cloud Function:\\\n",
    "   gcloud pubsub topics create kobo-extractor-trigger\n",
    "   \n",
    "2. Trrigger despliega una funci√≥n que se ejecuta autom√°ticamente cuando llega un mensaje al topic: \\\n",
    "   gcloud functions deploy extraer_datos_kobo \\\n",
    "  --runtime python311 \\\n",
    "  --trigger-topic kobo-extractor-trigger \\\n",
    "  --region=us-central1\n",
    "\n",
    "   \n",
    "4. Programar la tarea: \\\n",
    "   gcloud scheduler jobs create pubsub ejecutar-extraccion-kobo \\\n",
    "  --schedule=\"0 8 1 * *\" \\\n",
    "  --topic=kobo-extractor-trigger \\\n",
    "  --message-body=\"Iniciar extracci√≥n de KoboToolbox\" \\\n",
    "  --time-zone=\"America/Mexico_City\"\n",
    "\n",
    "\n",
    "----------------------\n",
    "\n",
    "\n",
    "### üìã Atomizaci√≥n de datos\n",
    "Una vez generada la dase de datos cruda en Cloud Storage se despliega otra funci√≥n donde a partir de los datos crudos se haga la atomizaci√≥n de los registros y se genere el Query. Su respectivo requirements.txt debe de contener: \n",
    "1) google-cloud-storage\n",
    "2) google-cloud-bigquery\n",
    "\n",
    "\n",
    "#### üöÄ C√≥digo *main.py*\n",
    "```` python\n",
    "    import json\n",
    "    from google.cloud import storage, bigquery\n",
    "\n",
    "    def atomizar_y_cargar(event, context):\n",
    "        \"\"\"Se ejecuta autom√°ticamente cuando se sube un nuevo archivo al bucket.\"\"\"\n",
    "        bucket_name = event['bucket']\n",
    "        file_name = event['name']\n",
    "        print(f\"Nuevo archivo detectado: gs://{bucket_name}/{file_name}\")\n",
    "\n",
    "        # --- Leer el archivo desde el bucket ---\n",
    "        \n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name) \n",
    "        blob = bucket.blob(file_name) # Referencia al archivo espec√≠fico\n",
    "        raw_data = blob.download_as_text()\n",
    "        json_data = json.loads(raw_data) #Convierte el texto JSON a objeto Python\n",
    "\n",
    "        # --- Extraer los registros del JSON ---\n",
    "        registros = json_data.get(\"results\", [])\n",
    "\n",
    "        # --- Limpiar y estructurar los datos ---\n",
    "        data_limpia = []\n",
    "        \n",
    "        for item in registros:\n",
    "            # Extraer datos del grupo \"group_dr3zv09\" (Informaci√≥n general)\n",
    "            grupo_principal = item.get(\"group_dr3zv09\", {})\n",
    "        \n",
    "            # Extraer datos del grupo \"group_jf5li71\" (Informaci√≥n de calidad de agua)\n",
    "            grupo_agua = item.get(\"group_kv0zx70\", {})\n",
    "            data_limpia.append({\n",
    "                \n",
    "                # --- Informaci√≥n general (grupo_dr3zv09) ---\n",
    "                \"nombre_completo\": grupo_principal.get(\"Nombre_Completo\"),\n",
    "                \"fecha_recoleccion\": grupo_principal.get(\"Fecha_y_hora_de_recolecci_n\"),\n",
    "                \"reserva\": grupo_principal.get(\"reserva\"),\n",
    "                \"sitio\": grupo_principal.get(\"sitio\"),\n",
    "                \"codigo_sitio\": grupo_principal.get(\"codigo\"),\n",
    "                \n",
    "                # --- Informaci√≥n de calidad de agua (grupo_kv0zx70) ---\n",
    "                \"Temperatura\": grupo_agua.get(\"Temp_C\"),  \n",
    "                \"Salinidad\": grupo_agua.get(\"Salinidad_PSU\"),\n",
    "                \"Conductividad\": grupo_sensor.get(\"Conductividad_mS_cm\"),\n",
    "\n",
    "        })\n",
    "\n",
    "\n",
    "        # --- Subir los datos a BigQuery ---\n",
    "        bq_client = bigquery.Client()\n",
    "        table_id = \"Parametros_agua_atomizado\"\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_APPEND\",  # agrega datos nuevos sin borrar los anteriores\n",
    "        autodetect=True # BigQuery infiere autom√°ticamente el esque de datos\n",
    "        )\n",
    "\n",
    "        job = bq_client.load_table_from_json(data_limpia, table_id, job_config=job_config)\n",
    "        job.result()  # Esperar a que finalice la carga, dtiene la ejecuci√≥n en CloudFunctions\n",
    "\n",
    "        print(f\"{len(data_limpia)} registros cargados a {table_id}\")\n",
    "\n",
    "````\n",
    "\n",
    "Desde Terminal se gener√° el ***Disparador*** de la funci√≥n para que se relice de forma automatica cada vez que se detecten nuevos datos en el bucket\n",
    "\n",
    "````pyhton \n",
    "gcloud functions deploy atomizar_y_cargar \\\n",
    "  --runtime python311 \n",
    "  --trigger-resource Par√°metros_de_Agua \\\n",
    "  --trigger-event google.storage.object.finalize \\\n",
    "  --region=us-central1\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e47f7c6-79f7-4581-979d-aa09a5020654",
   "metadata": {},
   "source": [
    "## üìÅ C√°maras Terrestre\n",
    "--------\n",
    "\n",
    "### ‚ö° Descarga de datos crudos\n",
    "- *requirement.txt* \n",
    "\n",
    " \n",
    "\n",
    "1) requests ‚Üí Para hacer solicitudes HTTP a la API de KoBoToolbox.\n",
    "\n",
    "2) google.cloud.storage ‚Üí Para conectarse al Bucket de Cloud Storage.\n",
    "\n",
    "3) functions_framework ‚Üí Permite ejecutar la funci√≥n como Cloud Function.\n",
    "\n",
    "Para configurar la funci√≥n en Google Cloud Functions (GCF), es necesario establecer las variables de entorno requeridas por el c√≥digo principal, lo que simplifica su despliegue y mantenimiento.\n",
    "\n",
    "- KOBO_TOKEN= \"d0e1084994983390bd6f50e3ee61e9c522aee152\"\n",
    "- BUCKET_NAME= C√°maras_Terrestres\n",
    "#### üöÄ C√≥digo *main.py*\n",
    "```` python\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "import functions_framework\n",
    "\n",
    "@functions_framework.http # Permite que la funci√≥n se ejecute cuando recibe una petici√≥n web y convierte la funci√≥n en una Cloud Function HTTP\n",
    "def main(request):\n",
    "    \n",
    "    # --- Configuraci√≥n ---\n",
    "    KOBO_URL = \"https://kf.kobotoolbox.org/api/v2/assets/aMQsvne8A7orHTdj6Yfh5x/data.json\" #Endpoint espec√≠fico de la API de KoboToolbox para un asset/proyecto\n",
    "    KOBO_TOKEN = os.getenv(\"KOBO_TOKEN\")\n",
    "    BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "\n",
    "    # --- Descargar datos desde KoboToolbox ---\n",
    "    headers = {\"Authorization\": f\"Token {KOBO_TOKEN}\"} # Configura autenticaci√≥n Bearer Token para la API\n",
    "    response = requests.get(KOBO_URL, headers=headers) # Ejecuta la petici√≥n GET y almacena la respuesta\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return f\"Error {response.status_code}: {response.text}\", response.status_code\n",
    "\n",
    "    data = response.json() # Convertir respuesta a JSON\n",
    "\n",
    "    # --- Nombrar archivo por fecha ---\n",
    "    fecha = datetime.utcnow().strftime(\"%Y-%m-%dT%H%M%SZ\") # Se obtiene la fecha y se le da formato\n",
    "    filename = f\"kobo_raw_{fecha}.json\" # Se nombra el archivo\n",
    "\n",
    "    # --- Guardar archivo en Cloud Storage ---\n",
    "    \n",
    "    storage_client = storage.Client() # Crea una instancia del cliente oficial de Python para interactuar con Google Cloud Storage \n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob(filename) # Representa el archivo que se crear√° en el bucket\n",
    "    blob.upload_from_string(json.dumps(data), content_type=\"application/json\")\n",
    "\n",
    "    print(f\"Archivo subido: gs://{BUCKET_NAME}/{filename}\")\n",
    "    return f\"Datos guardados en {filename}\", 200\n",
    "\n",
    "````\n",
    "\n",
    "üîπPara realizar la automatizaci√≥n de la descarga de datos se tiene que declarar un mensaje de activaci√≥n, el disparador (trigger) y por ultimo se declaran las condiciones para que se ejecute en automatico. \n",
    "\n",
    "1. Mensaje de activaci√≥n funciona como intermediario entre Scheduler y Cloud Function:\\\n",
    "   gcloud pubsub topics create kobo-extractor-trigger\n",
    "   \n",
    "2. Trrigger despliega una funci√≥n que se ejecuta autom√°ticamente cuando llega un mensaje al topic: \\\n",
    "   gcloud functions deploy extraer_datos_kobo \\\n",
    "  --runtime python311 \\\n",
    "  --trigger-topic kobo-extractor-trigger \\\n",
    "  --region=us-central1\n",
    "\n",
    "   \n",
    "4. Programar la tarea: \\\n",
    "   gcloud scheduler jobs create pubsub ejecutar-extraccion-kobo \\\n",
    "  --schedule=\"0 8 1 * *\" \\\n",
    "  --topic=kobo-extractor-trigger \\\n",
    "  --message-body=\"Iniciar extracci√≥n de KoboToolbox\" \\\n",
    "  --time-zone=\"America/Mexico_City\"\n",
    "\n",
    "\n",
    "----------------------\n",
    "\n",
    "\n",
    "### üìã Atomizaci√≥n de datos\n",
    "Una vez generada la dase de datos cruda en Cloud Storage se despliega otra funci√≥n donde a partir de los datos crudos se haga la atomizaci√≥n de los registros y se genere el Query. Su respectivo requirements.txt debe de contener: \n",
    "1) google-cloud-storage\n",
    "2) google-cloud-bigquery\n",
    "\n",
    "\n",
    "#### üöÄ C√≥digo *main.py*\n",
    "```` python\n",
    "    import json\n",
    "    from google.cloud import storage, bigquery\n",
    "\n",
    "    def atomizar_y_cargar(event, context):\n",
    "        \"\"\"Se ejecuta autom√°ticamente cuando se sube un nuevo archivo al bucket.\"\"\"\n",
    "        bucket_name = event['bucket']\n",
    "        file_name = event['name']\n",
    "        print(f\"Nuevo archivo detectado: gs://{bucket_name}/{file_name}\")\n",
    "\n",
    "        # --- Leer el archivo desde el bucket ---\n",
    "        \n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name) \n",
    "        blob = bucket.blob(file_name) # Referencia al archivo espec√≠fico\n",
    "        raw_data = blob.download_as_text()\n",
    "        json_data = json.loads(raw_data) #Convierte el texto JSON a objeto Python\n",
    "\n",
    "        # --- Extraer los registros del JSON ---\n",
    "        registros = json_data.get(\"results\", [])\n",
    "\n",
    "        # --- Limpiar y estructurar los datos ---\n",
    "        data_limpia = []\n",
    "        \n",
    "        for item in registros:\n",
    "\n",
    "            # Extraer datos del grupo \"group_kv0zx70\" (Informaci√≥n de instalaci√≥n)\n",
    "             grupo_intalaci√≥n = item.get(\"group_kv0zx70\", {})\n",
    "\n",
    "            # Extraer datos del grupo \"group_dr3zv09\" (Informaci√≥n general)\n",
    "            grupo_principal = item.get(\"group_dr3zv09\", {})\n",
    "        \n",
    "            # Extraer datos del grupo \"group_rw9dd47/group_it8nx96\" (Informaci√≥n de OP)\n",
    "            grupo_op = item.get(\"group_kv0zx70\", {})\n",
    "            data_limpia.append({\n",
    "                \n",
    "                # --- Informaci√≥n general (grupo_dr3zv09) ---\n",
    "                \"nombre_completo\": grupo_principal.get(\"Nombre_Completo\"),\n",
    "                \"fecha_recoleccion\": grupo_principal.get(\"Fecha_y_hora_de_recolecci_n\"),\n",
    "                \"reserva\": grupo_principal.get(\"reserva\"),\n",
    "                \"sitio\": grupo_principal.get(\"sitio\"),\n",
    "                \"codigo_sitio\": grupo_principal.get(\"codigo\"),\n",
    "                \n",
    "                # --- Informaci√≥n de operaci√≥n y mantenimiento (group_rw9dd47/group_it8nx96) ---\n",
    "                \"Modo de Operaci√≥n\": grupo_op.get(\"Modo_de_operaci_n_001\"),  \n",
    "                \"Se√±al\": grupo_op.get(\"Intensidad_de_la_se_al\"),\n",
    "\n",
    "                # ---- Informaci√≥n de Instalaci√≥n (group_kv0zx70)\n",
    "\n",
    "                \"Altura del sensor\": grupo_instlaci√≥n.get(\"Altura_del_sensor_m_desde_el\"),  \n",
    "                \"Coordenadas\": grupo_op.get(\"Latitud_y_Longitud_del_sensor\"),\n",
    "\n",
    "        })\n",
    "\n",
    "\n",
    "        # --- Subir los datos a BigQuery ---\n",
    "        bq_client = bigquery.Client()\n",
    "        table_id = \"C√°maras_terrestre_atomizado\"\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_APPEND\",  # agrega datos nuevos sin borrar los anteriores\n",
    "        autodetect=True # BigQuery infiere autom√°ticamente el esque de datos\n",
    "        )\n",
    "\n",
    "        job = bq_client.load_table_from_json(data_limpia, table_id, job_config=job_config)\n",
    "        job.result()  # Esperar a que finalice la carga, dtiene la ejecuci√≥n en CloudFunctions\n",
    "\n",
    "        print(f\"{len(data_limpia)} registros cargados a {table_id}\")\n",
    "\n",
    "````\n",
    "\n",
    "Desde Terminal se gener√° el ***Disparador*** de la funci√≥n para que se relice de forma automatica cada vez que se detecten nuevos datos en el bucket\n",
    "\n",
    "````pyhton \n",
    "gcloud functions deploy atomizar_y_cargar \\\n",
    "  --runtime python311 \n",
    "  --trigger-resource C√°maras_Terrestres \\\n",
    "  --trigger-event google.storage.object.finalize \\\n",
    "  --region=us-central1\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d0fa3-6680-450d-bb4b-ae4a906a8810",
   "metadata": {},
   "source": [
    "## üìÅ Zooplancton\n",
    "--------\n",
    "\n",
    "### ‚ö° Descarga de datos crudos\n",
    "- *requirement.txt* \n",
    "\n",
    " \n",
    "\n",
    "1) requests ‚Üí Para hacer solicitudes HTTP a la API de KoBoToolbox.\n",
    "\n",
    "2) google.cloud.storage ‚Üí Para conectarse al Bucket de Cloud Storage.\n",
    "\n",
    "3) functions_framework ‚Üí Permite ejecutar la funci√≥n como Cloud Function.\n",
    "\n",
    "Para configurar la funci√≥n en Google Cloud Functions (GCF), es necesario establecer las variables de entorno requeridas por el c√≥digo principal, lo que simplifica su despliegue y mantenimiento.\n",
    "\n",
    "- KOBO_TOKEN= \"d0e1084994983390bd6f50e3ee61e9c522aee152\"\n",
    "- BUCKET_NAME= Zooplancton\n",
    "#### üöÄ C√≥digo *main.py*\n",
    "```` python\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "import functions_framework\n",
    "\n",
    "@functions_framework.http # Permite que la funci√≥n se ejecute cuando recibe una petici√≥n web y convierte la funci√≥n en una Cloud Function HTTP\n",
    "def main(request):\n",
    "    \n",
    "    # --- Configuraci√≥n ---\n",
    "    KOBO_URL = \"https://kf.kobotoolbox.org/api/v2/assets/aojT2JZfgdqjTsxNszRdVz/data.json\" #Endpoint espec√≠fico de la API de KoboToolbox para un asset/proyecto\n",
    "    KOBO_TOKEN = os.getenv(\"KOBO_TOKEN\")\n",
    "    BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "\n",
    "    # --- Descargar datos desde KoboToolbox ---\n",
    "    headers = {\"Authorization\": f\"Token {KOBO_TOKEN}\"} # Configura autenticaci√≥n Bearer Token para la API\n",
    "    response = requests.get(KOBO_URL, headers=headers) # Ejecuta la petici√≥n GET y almacena la respuesta\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return f\"Error {response.status_code}: {response.text}\", response.status_code\n",
    "\n",
    "    data = response.json() # Convertir respuesta a JSON\n",
    "\n",
    "    # --- Nombrar archivo por fecha ---\n",
    "    fecha = datetime.utcnow().strftime(\"%Y-%m-%dT%H%M%SZ\") # Se obtiene la fecha y se le da formato\n",
    "    filename = f\"kobo_raw_{fecha}.json\" # Se nombra el archivo\n",
    "\n",
    "    # --- Guardar archivo en Cloud Storage ---\n",
    "    \n",
    "    storage_client = storage.Client() # Crea una instancia del cliente oficial de Python para interactuar con Google Cloud Storage \n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob(filename) # Representa el archivo que se crear√° en el bucket\n",
    "    blob.upload_from_string(json.dumps(data), content_type=\"application/json\")\n",
    "\n",
    "    print(f\"Archivo subido: gs://{BUCKET_NAME}/{filename}\")\n",
    "    return f\"Datos guardados en {filename}\", 200\n",
    "\n",
    "````\n",
    "\n",
    "üîπPara realizar la automatizaci√≥n de la descarga de datos se tiene que declarar un mensaje de activaci√≥n, el disparador (trigger) y por ultimo se declaran las condiciones para que se ejecute en automatico. \n",
    "\n",
    "1. Mensaje de activaci√≥n funciona como intermediario entre Scheduler y Cloud Function:\\\n",
    "   gcloud pubsub topics create kobo-extractor-trigger\n",
    "   \n",
    "2. Trrigger despliega una funci√≥n que se ejecuta autom√°ticamente cuando llega un mensaje al topic: \\\n",
    "   gcloud functions deploy extraer_datos_kobo \\\n",
    "  --runtime python311 \\\n",
    "  --trigger-topic kobo-extractor-trigger \\\n",
    "  --region=us-central1\n",
    "\n",
    "   \n",
    "4. Programar la tarea: \\\n",
    "   gcloud scheduler jobs create pubsub ejecutar-extraccion-kobo \\\n",
    "  --schedule=\"0 8 1 * *\" \\\n",
    "  --topic=kobo-extractor-trigger \\\n",
    "  --message-body=\"Iniciar extracci√≥n de KoboToolbox\" \\\n",
    "  --time-zone=\"America/Mexico_City\"\n",
    "\n",
    "\n",
    "----------------------\n",
    "\n",
    "\n",
    "### üìã Atomizaci√≥n de datos\n",
    "Una vez generada la dase de datos cruda en Cloud Storage se despliega otra funci√≥n donde a partir de los datos crudos se haga la atomizaci√≥n de los registros y se genere el Query. Su respectivo requirements.txt debe de contener: \n",
    "1) google-cloud-storage\n",
    "2) google-cloud-bigquery\n",
    "\n",
    "\n",
    "#### üöÄ C√≥digo *main.py*\n",
    "```` python\n",
    "    import json\n",
    "    from google.cloud import storage, bigquery\n",
    "\n",
    "    def atomizar_y_cargar(event, context):\n",
    "        \"\"\"Se ejecuta autom√°ticamente cuando se sube un nuevo archivo al bucket.\"\"\"\n",
    "        bucket_name = event['bucket']\n",
    "        file_name = event['name']\n",
    "        print(f\"Nuevo archivo detectado: gs://{bucket_name}/{file_name}\")\n",
    "\n",
    "        # --- Leer el archivo desde el bucket ---\n",
    "        \n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name) \n",
    "        blob = bucket.blob(file_name) # Referencia al archivo espec√≠fico\n",
    "        raw_data = blob.download_as_text()\n",
    "        json_data = json.loads(raw_data) #Convierte el texto JSON a objeto Python\n",
    "\n",
    "        # --- Extraer los registros del JSON ---\n",
    "        registros = json_data.get(\"results\", [])\n",
    "\n",
    "        # --- Limpiar y estructurar los datos ---\n",
    "        data_limpia = []\n",
    "        \n",
    "        for item in registros:\n",
    "            # Extraer datos del grupo \"group_kv0zx70\" (Informaci√≥n de la muestra)\n",
    "            grupo_muestra = item.get(\"group_kv0zx70\", {})\n",
    "        \n",
    "            # Extraer datos del grupo \"group_jf5li71\" (Informaci√≥n de registro CTD)\n",
    "            grupo_registro = item.get(\"group_zq1vg22\", {})\n",
    "            data_limpia.append({\n",
    "                \n",
    "                # --- Informaci√≥n de la muestras (group_kv0zx70) ---\n",
    "                \"ID Muestra\": grupo_muestra.get(\"ID_Muestra\"),\n",
    "                \"Estado de la Marea\": grupo_muestra.get(\"Estado_de_la_marea\"),\n",
    "            \n",
    "                \n",
    "                # --- Informaci√≥n de registro de CTD (group_zq1vg22) ---\n",
    "                \"Temperatura\": grupo_registro.get(\"Temp_C\"),  \n",
    "                \"Salinidad\": grupo_registro.get(\"Salinidad_ppt\"),\n",
    "                \"Profundidad\": grupo_registro.get(\"Profundidad_m\"),\n",
    "\n",
    "        })\n",
    "\n",
    "\n",
    "        # --- Subir los datos a BigQuery ---\n",
    "        bq_client = bigquery.Client()\n",
    "        table_id = \"Zooplancton_atomizado\"\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_APPEND\",  # agrega datos nuevos sin borrar los anteriores\n",
    "        autodetect=True # BigQuery infiere autom√°ticamente el esque de datos\n",
    "        )\n",
    "\n",
    "        job = bq_client.load_table_from_json(data_limpia, table_id, job_config=job_config)\n",
    "        job.result()  # Esperar a que finalice la carga, dtiene la ejecuci√≥n en CloudFunctions\n",
    "\n",
    "        print(f\"{len(data_limpia)} registros cargados a {table_id}\")\n",
    "\n",
    "````\n",
    "\n",
    "Desde Terminal se gener√° el ***Disparador*** de la funci√≥n para que se relice de forma automatica cada vez que se detecten nuevos datos en el bucket\n",
    "\n",
    "````pyhton \n",
    "gcloud functions deploy atomizar_y_cargar \\\n",
    "  --runtime python311 \n",
    "  --trigger-resource Zooplancton \\\n",
    "  --trigger-event google.storage.object.finalize \\\n",
    "  --region=us-central1\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ae4b2c-7bc9-4ea7-bc06-22811b366790",
   "metadata": {},
   "source": [
    "## üìÅ Vuelo de dron\n",
    "--------\n",
    "\n",
    "### ‚ö° Descarga de datos crudos\n",
    "- *requirement.txt* \n",
    "\n",
    " \n",
    "\n",
    "1) requests ‚Üí Para hacer solicitudes HTTP a la API de KoBoToolbox.\n",
    "\n",
    "2) google.cloud.storage ‚Üí Para conectarse al Bucket de Cloud Storage.\n",
    "\n",
    "3) functions_framework ‚Üí Permite ejecutar la funci√≥n como Cloud Function.\n",
    "\n",
    "Para configurar la funci√≥n en Google Cloud Functions (GCF), es necesario establecer las variables de entorno requeridas por el c√≥digo principal, lo que simplifica su despliegue y mantenimiento.\n",
    "\n",
    "- KOBO_TOKEN= \"d0e1084994983390bd6f50e3ee61e9c522aee152\"\n",
    "- BUCKET_NAME= Vuelo_de_dron\n",
    "#### üöÄ C√≥digo *main.py*\n",
    "```` python\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "import functions_framework\n",
    "\n",
    "@functions_framework.http # Permite que la funci√≥n se ejecute cuando recibe una petici√≥n web y convierte la funci√≥n en una Cloud Function HTTP\n",
    "def main(request):\n",
    "    \n",
    "    # --- Configuraci√≥n ---\n",
    "    KOBO_URL = \"https://kf.kobotoolbox.org/api/v2/assets/aBwshXwrJ77EyRLtrx2s8z/data.json\" #Endpoint espec√≠fico de la API de KoboToolbox para un asset/proyecto\n",
    "    KOBO_TOKEN = os.getenv(\"KOBO_TOKEN\")\n",
    "    BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "\n",
    "    # --- Descargar datos desde KoboToolbox ---\n",
    "    headers = {\"Authorization\": f\"Token {KOBO_TOKEN}\"} # Configura autenticaci√≥n Bearer Token para la API\n",
    "    response = requests.get(KOBO_URL, headers=headers) # Ejecuta la petici√≥n GET y almacena la respuesta\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return f\"Error {response.status_code}: {response.text}\", response.status_code\n",
    "\n",
    "    data = response.json() # Convertir respuesta a JSON\n",
    "\n",
    "    # --- Nombrar archivo por fecha ---\n",
    "    fecha = datetime.utcnow().strftime(\"%Y-%m-%dT%H%M%SZ\") # Se obtiene la fecha y se le da formato\n",
    "    filename = f\"kobo_raw_{fecha}.json\" # Se nombra el archivo\n",
    "\n",
    "    # --- Guardar archivo en Cloud Storage ---\n",
    "    \n",
    "    storage_client = storage.Client() # Crea una instancia del cliente oficial de Python para interactuar con Google Cloud Storage \n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob(filename) # Representa el archivo que se crear√° en el bucket\n",
    "    blob.upload_from_string(json.dumps(data), content_type=\"application/json\")\n",
    "\n",
    "    print(f\"Archivo subido: gs://{BUCKET_NAME}/{filename}\")\n",
    "    return f\"Datos guardados en {filename}\", 200\n",
    "\n",
    "````\n",
    "\n",
    "üîπPara realizar la automatizaci√≥n de la descarga de datos se tiene que declarar un mensaje de activaci√≥n, el disparador (trigger) y por ultimo se declaran las condiciones para que se ejecute en automatico. \n",
    "\n",
    "1. Mensaje de activaci√≥n funciona como intermediario entre Scheduler y Cloud Function:\\\n",
    "   gcloud pubsub topics create kobo-extractor-trigger\n",
    "   \n",
    "2. Trrigger despliega una funci√≥n que se ejecuta autom√°ticamente cuando llega un mensaje al topic: \\\n",
    "   gcloud functions deploy extraer_datos_kobo \\\n",
    "  --runtime python311 \\\n",
    "  --trigger-topic kobo-extractor-trigger \\\n",
    "  --region=us-central1\n",
    "\n",
    "   \n",
    "4. Programar la tarea: \\\n",
    "   gcloud scheduler jobs create pubsub ejecutar-extraccion-kobo \\\n",
    "  --schedule=\"0 8 1 * *\" \\\n",
    "  --topic=kobo-extractor-trigger \\\n",
    "  --message-body=\"Iniciar extracci√≥n de KoboToolbox\" \\\n",
    "  --time-zone=\"America/Mexico_City\"\n",
    "\n",
    "\n",
    "----------------------\n",
    "\n",
    "\n",
    "### üìã Atomizaci√≥n de datos\n",
    "Una vez generada la dase de datos cruda en Cloud Storage se despliega otra funci√≥n donde a partir de los datos crudos se haga la atomizaci√≥n de los registros y se genere el Query. Su respectivo requirements.txt debe de contener: \n",
    "1) google-cloud-storage\n",
    "2) google-cloud-bigquery\n",
    "\n",
    "\n",
    "#### üöÄ C√≥digo *main.py*\n",
    "```` python\n",
    "    import json\n",
    "    from google.cloud import storage, bigquery\n",
    "\n",
    "    def atomizar_y_cargar(event, context):\n",
    "        \"\"\"Se ejecuta autom√°ticamente cuando se sube un nuevo archivo al bucket.\"\"\"\n",
    "        bucket_name = event['bucket']\n",
    "        file_name = event['name']\n",
    "        print(f\"Nuevo archivo detectado: gs://{bucket_name}/{file_name}\")\n",
    "\n",
    "        # --- Leer el archivo desde el bucket ---\n",
    "        \n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name) \n",
    "        blob = bucket.blob(file_name) # Referencia al archivo espec√≠fico\n",
    "        raw_data = blob.download_as_text()\n",
    "        json_data = json.loads(raw_data) #Convierte el texto JSON a objeto Python\n",
    "\n",
    "        # --- Extraer los registros del JSON ---\n",
    "        registros = json_data.get(\"results\", [])\n",
    "\n",
    "        # --- Limpiar y estructurar los datos ---\n",
    "        data_limpia = []\n",
    "        \n",
    "        for item in registros:\n",
    "            # Extraer datos del grupo \"group_dr3zv09\" (Informaci√≥n general)\n",
    "            grupo_principal = item.get(\"group_dr3zv09\", {})\n",
    "        \n",
    "            # Extraer datos del grupo \"group_kv0zx70\" (Informaci√≥n del vuelo de dron)\n",
    "            grupo_vuelo = item.get(\"group_kv0zx70\", {})\n",
    "            data_limpia.append({\n",
    "                \n",
    "                # --- Informaci√≥n general (grupo_dr3zv09) ---\n",
    "                \"nombre_completo\": grupo_principal.get(\"Nombre_Completo\"),\n",
    "                \"fecha_recoleccion\": grupo_principal.get(\"Fecha_y_hora_de_recolecci_n\"),\n",
    "                \"reserva\": grupo_principal.get(\"reserva\"),\n",
    "                \"sitio\": grupo_principal.get(\"sitio\"),\n",
    "                \"codigo_sitio\": grupo_principal.get(\"codigo\"),\n",
    "                \n",
    "                # --- Informaci√≥n de calidad de agua (grupo_kv0zx70) ---\n",
    "                \"Tipo de sensor\": item.get(\"Tipo_de_sensor\"),  # Fuera de grupo\n",
    "                \"Indice\": grupo_vuelo.get(\"_ndice\"),\n",
    "                \"Conductividad\": grupo_vuelo.get(\"Conductividad_mS_cm\"),\n",
    "\n",
    "        })\n",
    "\n",
    "\n",
    "        # --- Subir los datos a BigQuery ---\n",
    "        bq_client = bigquery.Client()\n",
    "        table_id = \"Vuelo_de_dron_atomizado\"\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_APPEND\",  # agrega datos nuevos sin borrar los anteriores\n",
    "        autodetect=True # BigQuery infiere autom√°ticamente el esque de datos\n",
    "        )\n",
    "\n",
    "        job = bq_client.load_table_from_json(data_limpia, table_id, job_config=job_config)\n",
    "        job.result()  # Esperar a que finalice la carga, dtiene la ejecuci√≥n en CloudFunctions\n",
    "\n",
    "        print(f\"{len(data_limpia)} registros cargados a {table_id}\")\n",
    "\n",
    "````\n",
    "\n",
    "Desde Terminal se gener√° el ***Disparador*** de la funci√≥n para que se relice de forma automatica cada vez que se detecten nuevos datos en el bucket\n",
    "\n",
    "````pyhton \n",
    "gcloud functions deploy atomizar_y_cargar \\\n",
    "  --runtime python311 \n",
    "  --trigger-resource Vuelo_de_dron \\\n",
    "  --trigger-event google.storage.object.finalize \\\n",
    "  --region=us-central1\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3180ca-7799-4f31-a671-dbda64b0371e",
   "metadata": {},
   "source": [
    "## üìÅ Estaciones Meteorol√≥gicas\n",
    "--------\n",
    "\n",
    "### ‚ö° Descarga de datos crudos\n",
    "- *requirement.txt* \n",
    "\n",
    " \n",
    "\n",
    "1) requests ‚Üí Para hacer solicitudes HTTP a la API de KoBoToolbox.\n",
    "\n",
    "2) google.cloud.storage ‚Üí Para conectarse al Bucket de Cloud Storage.\n",
    "\n",
    "3) functions_framework ‚Üí Permite ejecutar la funci√≥n como Cloud Function.\n",
    "\n",
    "Para configurar la funci√≥n en Google Cloud Functions (GCF), es necesario establecer las variables de entorno requeridas por el c√≥digo principal, lo que simplifica su despliegue y mantenimiento.\n",
    "\n",
    "- KOBO_TOKEN= \"d0e1084994983390bd6f50e3ee61e9c522aee152\"\n",
    "- BUCKET_NAME= Estaciones_Meteorol√≥gicas \n",
    "#### üöÄ C√≥digo *main.py*\n",
    "```` python\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "import functions_framework\n",
    "\n",
    "@functions_framework.http # Permite que la funci√≥n se ejecute cuando recibe una petici√≥n web y convierte la funci√≥n en una Cloud Function HTTP\n",
    "def main(request):\n",
    "    \n",
    "    # --- Configuraci√≥n ---\n",
    "    KOBO_URL = \"https://kf.kobotoolbox.org/api/v2/assets/an57S6QY6v3RwTWDnM2ev4/data.json\" #Endpoint espec√≠fico de la API de KoboToolbox para un asset/proyecto\n",
    "    KOBO_TOKEN = os.getenv(\"KOBO_TOKEN\")\n",
    "    BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "\n",
    "    # --- Descargar datos desde KoboToolbox ---\n",
    "    headers = {\"Authorization\": f\"Token {KOBO_TOKEN}\"} # Configura autenticaci√≥n Bearer Token para la API\n",
    "    response = requests.get(KOBO_URL, headers=headers) # Ejecuta la petici√≥n GET y almacena la respuesta\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return f\"Error {response.status_code}: {response.text}\", response.status_code\n",
    "\n",
    "    data = response.json() # Convertir respuesta a JSON\n",
    "\n",
    "    # --- Nombrar archivo por fecha ---\n",
    "    fecha = datetime.utcnow().strftime(\"%Y-%m-%dT%H%M%SZ\") # Se obtiene la fecha y se le da formato\n",
    "    filename = f\"kobo_raw_{fecha}.json\" # Se nombra el archivo\n",
    "\n",
    "    # --- Guardar archivo en Cloud Storage ---\n",
    "    \n",
    "    storage_client = storage.Client() # Crea una instancia del cliente oficial de Python para interactuar con Google Cloud Storage \n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob(filename) # Representa el archivo que se crear√° en el bucket\n",
    "    blob.upload_from_string(json.dumps(data), content_type=\"application/json\")\n",
    "\n",
    "    print(f\"Archivo subido: gs://{BUCKET_NAME}/{filename}\")\n",
    "    return f\"Datos guardados en {filename}\", 200\n",
    "\n",
    "````\n",
    "\n",
    "üîπPara realizar la automatizaci√≥n de la descarga de datos se tiene que declarar un mensaje de activaci√≥n, el disparador (trigger) y por ultimo se declaran las condiciones para que se ejecute en automatico. \n",
    "\n",
    "1. Mensaje de activaci√≥n funciona como intermediario entre Scheduler y Cloud Function:\\\n",
    "   gcloud pubsub topics create kobo-extractor-trigger\n",
    "   \n",
    "2. Trrigger despliega una funci√≥n que se ejecuta autom√°ticamente cuando llega un mensaje al topic: \\\n",
    "   gcloud functions deploy extraer_datos_kobo \\\n",
    "  --runtime python311 \\\n",
    "  --trigger-topic kobo-extractor-trigger \\\n",
    "  --region=us-central1\n",
    "\n",
    "   \n",
    "4. Programar la tarea: \\\n",
    "   gcloud scheduler jobs create pubsub ejecutar-extraccion-kobo \\\n",
    "  --schedule=\"0 8 1 * *\" \\\n",
    "  --topic=kobo-extractor-trigger \\\n",
    "  --message-body=\"Iniciar extracci√≥n de KoboToolbox\" \\\n",
    "  --time-zone=\"America/Mexico_City\"\n",
    "\n",
    "\n",
    "----------------------\n",
    "\n",
    "\n",
    "### üìã Atomizaci√≥n de datos\n",
    "Una vez generada la dase de datos cruda en Cloud Storage se despliega otra funci√≥n donde a partir de los datos crudos se haga la atomizaci√≥n de los registros y se genere el Query. Su respectivo requirements.txt debe de contener: \n",
    "1) google-cloud-storage\n",
    "2) google-cloud-bigquery\n",
    "\n",
    "\n",
    "#### üöÄ C√≥digo *main.py*\n",
    "```` python\n",
    "    import json\n",
    "    from google.cloud import storage, bigquery\n",
    "\n",
    "    def atomizar_y_cargar(event, context):\n",
    "        \"\"\"Se ejecuta autom√°ticamente cuando se sube un nuevo archivo al bucket.\"\"\"\n",
    "        bucket_name = event['bucket']\n",
    "        file_name = event['name']\n",
    "        print(f\"Nuevo archivo detectado: gs://{bucket_name}/{file_name}\")\n",
    "\n",
    "        # --- Leer el archivo desde el bucket ---\n",
    "        \n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name) \n",
    "        blob = bucket.blob(file_name) # Referencia al archivo espec√≠fico\n",
    "        raw_data = blob.download_as_text()\n",
    "        json_data = json.loads(raw_data) #Convierte el texto JSON a objeto Python\n",
    "\n",
    "        # --- Extraer los registros del JSON ---\n",
    "        registros = json_data.get(\"results\", [])\n",
    "\n",
    "        # --- Limpiar y estructurar los datos ---\n",
    "        data_limpia = []\n",
    "        \n",
    "        for item in registros:\n",
    "            # Extraer datos del grupo \"group_dr3zv09\" (Informaci√≥n general)\n",
    "            grupo_principal = item.get(\"group_dr3zv09\", {})\n",
    "        \n",
    "            # Extraer datos del grupo \"group_jf5li71\" (Informaci√≥n de calidad de agua)\n",
    "            grupo_instalacion = item.get(\"group_kv0zx70\", {})\n",
    "            data_limpia.append({\n",
    "                \n",
    "                # --- Informaci√≥n general (grupo_dr3zv09) ---\n",
    "                \"nombre_completo\": grupo_principal.get(\"Nombre_Completo\"),\n",
    "                \"fecha_recoleccion\": grupo_principal.get(\"Fecha_y_hora_de_recolecci_n\"),\n",
    "                \"reserva\": grupo_principal.get(\"reserva\"),\n",
    "                \"sitio\": grupo_principal.get(\"sitio\"),\n",
    "                \"codigo_sitio\": grupo_principal.get(\"codigo\"),\n",
    "                \n",
    "                # --- Informaci√≥n de instalaci√≥n de sensor (grupo_kv0zx70) ---\n",
    "                \"Tipo de sensor\": item.get(\"Tipo_de_sensor\"),  # Fuera de grupo\n",
    "                \"Coordenadas\": grupo_instalacion.get(\"Latitud_y_Longitud_del_sensor\"),\n",
    "                \"Altura del sensor\": grupo_instalacion.get(\"Altura_del_sensor_m_desde_el\"),\n",
    "\n",
    "        })\n",
    "\n",
    "\n",
    "        # --- Subir los datos a BigQuery ---\n",
    "        bq_client = bigquery.Client()\n",
    "        table_id = \"Estaciones_meteorol√≥gicas_atomizado\"\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_APPEND\",  # agrega datos nuevos sin borrar los anteriores\n",
    "        autodetect=True # BigQuery infiere autom√°ticamente el esque de datos\n",
    "        )\n",
    "\n",
    "        job = bq_client.load_table_from_json(data_limpia, table_id, job_config=job_config)\n",
    "        job.result()  # Esperar a que finalice la carga, dtiene la ejecuci√≥n en CloudFunctions\n",
    "\n",
    "        print(f\"{len(data_limpia)} registros cargados a {table_id}\")\n",
    "\n",
    "````\n",
    "\n",
    "Desde Terminal se gener√° el ***Disparador*** de la funci√≥n para que se relice de forma automatica cada vez que se detecten nuevos datos en el bucket\n",
    "\n",
    "````pyhton \n",
    "gcloud functions deploy atomizar_y_cargar \\\n",
    "  --runtime python311 \n",
    "  --trigger-resource Estaciones_Meteorol√≥gicas \\\n",
    "  --trigger-event google.storage.object.finalize \\\n",
    "  --region=us-central1\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b95522-37f7-4e31-a5f7-b13317840e8b",
   "metadata": {},
   "source": [
    "## üìÅ CTD's\n",
    "--------\n",
    "\n",
    "### ‚ö° Descarga de datos crudos\n",
    "- *requirement.txt* \n",
    "\n",
    " \n",
    "\n",
    "1) requests ‚Üí Para hacer solicitudes HTTP a la API de KoBoToolbox.\n",
    "\n",
    "2) google.cloud.storage ‚Üí Para conectarse al Bucket de Cloud Storage.\n",
    "\n",
    "3) functions_framework ‚Üí Permite ejecutar la funci√≥n como Cloud Function.\n",
    "\n",
    "Para configurar la funci√≥n en Google Cloud Functions (GCF), es necesario establecer las variables de entorno requeridas por el c√≥digo principal, lo que simplifica su despliegue y mantenimiento.\n",
    "\n",
    "- KOBO_TOKEN= \"d0e1084994983390bd6f50e3ee61e9c522aee152\"\n",
    "- BUCKET_NAME= CTDS\n",
    "#### üöÄ C√≥digo *main.py*\n",
    "```` python\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "import functions_framework\n",
    "\n",
    "@functions_framework.http # Permite que la funci√≥n se ejecute cuando recibe una petici√≥n web y convierte la funci√≥n en una Cloud Function HTTP\n",
    "def main(request):\n",
    "    \n",
    "    # --- Configuraci√≥n ---\n",
    "    KOBO_URL = \"https://kf.kobotoolbox.org/api/v2/assets/aAjboEpfSEbFjTZBxF8CKR/data.json\" #Endpoint espec√≠fico de la API de KoboToolbox para un asset/proyecto\n",
    "    KOBO_TOKEN = os.getenv(\"KOBO_TOKEN\")\n",
    "    BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "\n",
    "    # --- Descargar datos desde KoboToolbox ---\n",
    "    headers = {\"Authorization\": f\"Token {KOBO_TOKEN}\"} # Configura autenticaci√≥n Bearer Token para la API\n",
    "    response = requests.get(KOBO_URL, headers=headers) # Ejecuta la petici√≥n GET y almacena la respuesta\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return f\"Error {response.status_code}: {response.text}\", response.status_code\n",
    "\n",
    "    data = response.json() # Convertir respuesta a JSON\n",
    "\n",
    "    # --- Nombrar archivo por fecha ---\n",
    "    fecha = datetime.utcnow().strftime(\"%Y-%m-%dT%H%M%SZ\") # Se obtiene la fecha y se le da formato\n",
    "    filename = f\"kobo_raw_{fecha}.json\" # Se nombra el archivo\n",
    "\n",
    "    # --- Guardar archivo en Cloud Storage ---\n",
    "    \n",
    "    storage_client = storage.Client() # Crea una instancia del cliente oficial de Python para interactuar con Google Cloud Storage \n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob(filename) # Representa el archivo que se crear√° en el bucket\n",
    "    blob.upload_from_string(json.dumps(data), content_type=\"application/json\")\n",
    "\n",
    "    print(f\"Archivo subido: gs://{BUCKET_NAME}/{filename}\")\n",
    "    return f\"Datos guardados en {filename}\", 200\n",
    "\n",
    "````\n",
    "\n",
    "üîπPara realizar la automatizaci√≥n de la descarga de datos se tiene que declarar un mensaje de activaci√≥n, el disparador (trigger) y por ultimo se declaran las condiciones para que se ejecute en automatico. \n",
    "\n",
    "1. Mensaje de activaci√≥n funciona como intermediario entre Scheduler y Cloud Function:\\\n",
    "   gcloud pubsub topics create kobo-extractor-trigger\n",
    "   \n",
    "2. Trrigger despliega una funci√≥n que se ejecuta autom√°ticamente cuando llega un mensaje al topic: \\\n",
    "   gcloud functions deploy extraer_datos_kobo \\\n",
    "  --runtime python311 \\\n",
    "  --trigger-topic kobo-extractor-trigger \\\n",
    "  --region=us-central1\n",
    "\n",
    "   \n",
    "4. Programar la tarea: \\\n",
    "   gcloud scheduler jobs create pubsub ejecutar-extraccion-kobo \\\n",
    "  --schedule=\"0 8 1 * *\" \\\n",
    "  --topic=kobo-extractor-trigger \\\n",
    "  --message-body=\"Iniciar extracci√≥n de KoboToolbox\" \\\n",
    "  --time-zone=\"America/Mexico_City\"\n",
    "\n",
    "\n",
    "----------------------\n",
    "\n",
    "\n",
    "### üìã Atomizaci√≥n de datos\n",
    "Una vez generada la dase de datos cruda en Cloud Storage se despliega otra funci√≥n donde a partir de los datos crudos se haga la atomizaci√≥n de los registros y se genere el Query. Su respectivo requirements.txt debe de contener: \n",
    "1) google-cloud-storage\n",
    "2) google-cloud-bigquery\n",
    "\n",
    "\n",
    "#### üöÄ C√≥digo *main.py*\n",
    "```` python\n",
    "    import json\n",
    "    from google.cloud import storage, bigquery\n",
    "\n",
    "    def atomizar_y_cargar(event, context):\n",
    "        \"\"\"Se ejecuta autom√°ticamente cuando se sube un nuevo archivo al bucket.\"\"\"\n",
    "        bucket_name = event['bucket']\n",
    "        file_name = event['name']\n",
    "        print(f\"Nuevo archivo detectado: gs://{bucket_name}/{file_name}\")\n",
    "\n",
    "        # --- Leer el archivo desde el bucket ---\n",
    "        \n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name) \n",
    "        blob = bucket.blob(file_name) # Referencia al archivo espec√≠fico\n",
    "        raw_data = blob.download_as_text()\n",
    "        json_data = json.loads(raw_data) #Convierte el texto JSON a objeto Python\n",
    "\n",
    "        # --- Extraer los registros del JSON ---\n",
    "        registros = json_data.get(\"results\", [])\n",
    "\n",
    "        # --- Limpiar y estructurar los datos ---\n",
    "        data_limpia = []\n",
    "        \n",
    "        for item in registros:\n",
    "            # Extraer datos del grupo \"group_dr3zv09\" (Informaci√≥n general)\n",
    "            grupo_principal = item.get(\"group_dr3zv09\", {})\n",
    "        \n",
    "            # Extraer datos del grupo \"group_wh77o68\" (Informaci√≥n de OP)\n",
    "            grupo_op = item.get(\"group_wh77o68\", {})\n",
    "            data_limpia.append({\n",
    "                \n",
    "                # --- Informaci√≥n general (grupo_dr3zv09) ---\n",
    "                \"nombre_completo\": grupo_principal.get(\"Nombre_Completo\"),\n",
    "                \"fecha_recoleccion\": grupo_principal.get(\"Fecha_y_hora_de_recolecci_n\"),\n",
    "                \"reserva\": grupo_principal.get(\"reserva\"),\n",
    "                \"sitio\": grupo_principal.get(\"sitio\"),\n",
    "                \"codigo_sitio\": grupo_principal.get(\"codigo\"),\n",
    "                \n",
    "                # --- Informaci√≥n de OP (group_wh77o68) ---\n",
    "                \"Tipo de sensor\": item.get(\"Tipo_de_sensor\"),  # Fuera de grupo\n",
    "                \"Mantenimiento a\": grupo_op.get(\"Mantenimiento_a\"),\n",
    "                \"¬øNecesita ser recuperado?\": grupo_op.get(\"_El_sensor_necesita_ser_recupe\"),\n",
    "\n",
    "        })\n",
    "\n",
    "\n",
    "        # --- Subir los datos a BigQuery ---\n",
    "        bq_client = bigquery.Client()\n",
    "        table_id = \"CTDS_atomizado\"\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_APPEND\",  # agrega datos nuevos sin borrar los anteriores\n",
    "        autodetect=True # BigQuery infiere autom√°ticamente el esque de datos\n",
    "        )\n",
    "\n",
    "        job = bq_client.load_table_from_json(data_limpia, table_id, job_config=job_config)\n",
    "        job.result()  # Esperar a que finalice la carga, dtiene la ejecuci√≥n en CloudFunctions\n",
    "\n",
    "        print(f\"{len(data_limpia)} registros cargados a {table_id}\")\n",
    "\n",
    "````\n",
    "\n",
    "Desde Terminal se gener√° el ***Disparador*** de la funci√≥n para que se relice de forma automatica cada vez que se detecten nuevos datos en el bucket\n",
    "\n",
    "````pyhton \n",
    "gcloud functions deploy atomizar_y_cargar \\\n",
    "  --runtime python311 \n",
    "  --trigger-resource CTDS \\\n",
    "  --trigger-event google.storage.object.finalize \\\n",
    "  --region=us-central1\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618aea47-1c2f-4212-8637-d40cf7be227f",
   "metadata": {},
   "source": [
    "## üìÅ Estructura Forestal \n",
    "--------\n",
    "\n",
    "### ‚ö° Descarga de datos crudos\n",
    "- *requirement.txt* \n",
    "\n",
    " \n",
    "\n",
    "1) requests ‚Üí Para hacer solicitudes HTTP a la API de KoBoToolbox.\n",
    "\n",
    "2) google.cloud.storage ‚Üí Para conectarse al Bucket de Cloud Storage.\n",
    "\n",
    "3) functions_framework ‚Üí Permite ejecutar la funci√≥n como Cloud Function.\n",
    "\n",
    "Para configurar la funci√≥n en Google Cloud Functions (GCF), es necesario establecer las variables de entorno requeridas por el c√≥digo principal, lo que simplifica su despliegue y mantenimiento.\n",
    "\n",
    "- KOBO_TOKEN= \"d0e1084994983390bd6f50e3ee61e9c522aee152\"\n",
    "- BUCKET_NAME= Estructura_forestal\n",
    "#### üöÄ C√≥digo *main.py*\n",
    "```` python\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "import functions_framework\n",
    "\n",
    "@functions_framework.http # Permite que la funci√≥n se ejecute cuando recibe una petici√≥n web y convierte la funci√≥n en una Cloud Function HTTP\n",
    "def main(request):\n",
    "    \n",
    "    # --- Configuraci√≥n ---\n",
    "    KOBO_URL = \"https://kf.kobotoolbox.org/api/v2/assets/aenA9VYsH68Csi8QkDCntr/data.json\" #Endpoint espec√≠fico de la API de KoboToolbox para un asset/proyecto\n",
    "    KOBO_TOKEN = os.getenv(\"KOBO_TOKEN\")\n",
    "    BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "\n",
    "    # --- Descargar datos desde KoboToolbox ---\n",
    "    headers = {\"Authorization\": f\"Token {KOBO_TOKEN}\"} # Configura autenticaci√≥n Bearer Token para la API\n",
    "    response = requests.get(KOBO_URL, headers=headers) # Ejecuta la petici√≥n GET y almacena la respuesta\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return f\"Error {response.status_code}: {response.text}\", response.status_code\n",
    "\n",
    "    data = response.json() # Convertir respuesta a JSON\n",
    "\n",
    "    # --- Nombrar archivo por fecha ---\n",
    "    fecha = datetime.utcnow().strftime(\"%Y-%m-%dT%H%M%SZ\") # Se obtiene la fecha y se le da formato\n",
    "    filename = f\"kobo_raw_{fecha}.json\" # Se nombra el archivo\n",
    "\n",
    "    # --- Guardar archivo en Cloud Storage ---\n",
    "    \n",
    "    storage_client = storage.Client() # Crea una instancia del cliente oficial de Python para interactuar con Google Cloud Storage \n",
    "    bucket = storage_client.bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob(filename) # Representa el archivo que se crear√° en el bucket\n",
    "    blob.upload_from_string(json.dumps(data), content_type=\"application/json\")\n",
    "\n",
    "    print(f\"Archivo subido: gs://{BUCKET_NAME}/{filename}\")\n",
    "    return f\"Datos guardados en {filename}\", 200\n",
    "\n",
    "````\n",
    "\n",
    "üîπPara realizar la automatizaci√≥n de la descarga de datos se tiene que declarar un mensaje de activaci√≥n, el disparador (trigger) y por ultimo se declaran las condiciones para que se ejecute en automatico. \n",
    "\n",
    "1. Mensaje de activaci√≥n funciona como intermediario entre Scheduler y Cloud Function:\\\n",
    "   gcloud pubsub topics create kobo-extractor-trigger\n",
    "   \n",
    "2. Trrigger despliega una funci√≥n que se ejecuta autom√°ticamente cuando llega un mensaje al topic: \\\n",
    "   gcloud functions deploy extraer_datos_kobo \\\n",
    "  --runtime python311 \\\n",
    "  --trigger-topic kobo-extractor-trigger \\\n",
    "  --region=us-central1\n",
    "\n",
    "   \n",
    "4. Programar la tarea: \\\n",
    "   gcloud scheduler jobs create pubsub ejecutar-extraccion-kobo \\\n",
    "  --schedule=\"0 8 1 * *\" \\\n",
    "  --topic=kobo-extractor-trigger \\\n",
    "  --message-body=\"Iniciar extracci√≥n de KoboToolbox\" \\\n",
    "  --time-zone=\"America/Mexico_City\"\n",
    "\n",
    "\n",
    "----------------------\n",
    "\n",
    "\n",
    "### üìã Atomizaci√≥n de datos\n",
    "Una vez generada la dase de datos cruda en Cloud Storage se despliega otra funci√≥n donde a partir de los datos crudos se haga la atomizaci√≥n de los registros y se genere el Query. Su respectivo requirements.txt debe de contener: \n",
    "1) google-cloud-storage\n",
    "2) google-cloud-bigquery\n",
    "\n",
    "\n",
    "#### üöÄ C√≥digo *main.py*\n",
    "```` python\n",
    "    import json\n",
    "    from google.cloud import storage, bigquery\n",
    "\n",
    "    def atomizar_y_cargar(event, context):\n",
    "        \"\"\"Se ejecuta autom√°ticamente cuando se sube un nuevo archivo al bucket.\"\"\"\n",
    "        bucket_name = event['bucket']\n",
    "        file_name = event['name']\n",
    "        print(f\"Nuevo archivo detectado: gs://{bucket_name}/{file_name}\")\n",
    "\n",
    "        # --- Leer el archivo desde el bucket ---\n",
    "        \n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name) \n",
    "        blob = bucket.blob(file_name) # Referencia al archivo espec√≠fico\n",
    "        raw_data = blob.download_as_text()\n",
    "        json_data = json.loads(raw_data) #Convierte el texto JSON a objeto Python\n",
    "\n",
    "        # --- Extraer los registros del JSON ---\n",
    "        registros = json_data.get(\"results\", [])\n",
    "\n",
    "        # --- Limpiar y estructurar los datos ---\n",
    "        data_limpia = []\n",
    "        \n",
    "        for item in registros:\n",
    "            # Extraer datos del grupo \"group_dr3zv09\" (Informaci√≥n general)\n",
    "            grupo_principal = item.get(\"group_dr3zv09\", {})\n",
    "        \n",
    "            # Extraer datos del grupo \"group_rh5ho64/group_yh9gj88\" (Medidas de Arbol)\n",
    "            grupo_arbol = item.get(\"group_rh5ho64/group_yh9gj88\", {})\n",
    "            data_limpia.append({\n",
    "                \n",
    "                # --- Informaci√≥n general (grupo_dr3zv09) ---\n",
    "                \"nombre_completo\": grupo_principal.get(\"Nombre_Completo\"),\n",
    "                \"fecha_recoleccion\": grupo_principal.get(\"Fecha_y_hora_de_recolecci_n\"),\n",
    "                \"reserva\": grupo_principal.get(\"reserva\"),\n",
    "                \"sitio\": grupo_principal.get(\"sitio\"),\n",
    "                \"codigo_sitio\": grupo_principal.get(\"codigo\"),\n",
    "                \n",
    "                # --- Informaci√≥n de medidas de Arbol (group_wh77o68) ---\n",
    "                \"Tipo de encuesta\": item.get(\"Tipo_de_encuesta\"),  # Fuera de grupo\n",
    "                \"Especies Individuales\": grupo_arbol.get(\"Especies_individuales\"),\n",
    "                \"Altura del √°rbol\": grupo_op.get(\"Altura_del_individuo_m\"),\n",
    "\n",
    "        })\n",
    "\n",
    "\n",
    "        # --- Subir los datos a BigQuery ---\n",
    "        bq_client = bigquery.Client()\n",
    "        table_id = \"Estructura_forestal_atomizado\"\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=\"WRITE_APPEND\",  # agrega datos nuevos sin borrar los anteriores\n",
    "        autodetect=True # BigQuery infiere autom√°ticamente el esque de datos\n",
    "        )\n",
    "\n",
    "        job = bq_client.load_table_from_json(data_limpia, table_id, job_config=job_config)\n",
    "        job.result()  # Esperar a que finalice la carga, dtiene la ejecuci√≥n en CloudFunctions\n",
    "\n",
    "        print(f\"{len(data_limpia)} registros cargados a {table_id}\")\n",
    "\n",
    "````\n",
    "\n",
    "Desde Terminal se gener√° el ***Disparador*** de la funci√≥n para que se relice de forma automatica cada vez que se detecten nuevos datos en el bucket\n",
    "\n",
    "````pyhton \n",
    "gcloud functions deploy atomizar_y_cargar \\\n",
    "  --runtime python311 \n",
    "  --trigger-resource Estructura_forestal \\\n",
    "  --trigger-event google.storage.object.finalize \\\n",
    "  --region=us-central1\n",
    "````\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
